
import time
import json
from typing import List, Dict, Any, Tuple, Optional
from dataclasses import dataclass, asdict
from datetime import datetime
import os

from .metrics import EvaluationMetrics


@dataclass
class QueryTestCase:
    """
    ë‹¨ì¼ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
    
    Attributes:
        query: ì‚¬ìš©ìž ì§ˆë¬¸
        relevant_docs: ì •ë‹µ ë¬¸ì„œ ID ì§‘í•© (ground truth)
        relevant_scores: ë¬¸ì„œë³„ relevance score (NDCGìš©)
        expected_citations: ê¸°ëŒ€ë˜ëŠ” ì¸ìš© (ì˜ˆ: {'doc1#p10', 'doc2#p5'})
        category: ì§ˆë¬¸ ìœ í˜• (fact, comparison, multi-hop ë“±)
        domain: ë„ë©”ì¸ (medical, legal, academic, etc.)
    """
    query: str
    relevant_docs: List[str]
    relevant_scores: Optional[Dict[str, float]] = None
    expected_citations: Optional[List[str]] = None
    category: str = "general"
    domain: str = "general"
    
    def to_dict(self) -> Dict[str, Any]:
        return asdict(self)


@dataclass
class BenchmarkResult:
    """
    ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ê²°ê³¼
    
    Attributes:
        test_case: í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
        retrieved_docs: ì‹œìŠ¤í…œì´ ë°˜í™˜í•œ ë¬¸ì„œ IDë“¤
        generated_answer: ìƒì„±ëœ ë‹µë³€
        latency_ms: ì‘ë‹µ ì‹œê°„ (ë°€ë¦¬ì´ˆ)
        context_size: ì‚¬ìš©ëœ ì»¨í…ìŠ¤íŠ¸ í¬ê¸° (tokens)
        metrics: ê³„ì‚°ëœ ë©”íŠ¸ë¦­ë“¤
    """
    test_case: QueryTestCase
    system_name: str
    retrieved_docs: List[str]
    generated_answer: str
    latency_ms: float
    context_size: int
    metrics: Dict[str, float]
    timestamp: str = None
    
    def __post_init__(self):
        if self.timestamp is None:
            self.timestamp = datetime.now().isoformat()
    
    def to_dict(self) -> Dict[str, Any]:
        result = {
            'test_case': self.test_case.to_dict(),
            'system_name': self.system_name,
            'retrieved_docs': self.retrieved_docs,
            'generated_answer': self.generated_answer,
            'latency_ms': self.latency_ms,
            'context_size': self.context_size,
            'metrics': self.metrics,
            'timestamp': self.timestamp
        }
        return result


class BenchmarkFramework:
    """
    TreeRAG í‰ê°€ë¥¼ ìœ„í•œ ì¢…í•© ë²¤ì¹˜ë§ˆí¬ í”„ë ˆìž„ì›Œí¬
    
    Usage:
        framework = BenchmarkFramework()
        
        # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì¶”ê°€
        framework.add_test_case(QueryTestCase(
            query="ì¸ìŠë¦° ì €í•­ì„± ì¹˜ë£ŒëŠ”?",
            relevant_docs=['doc1_node5', 'doc1_node12'],
            relevant_scores={'doc1_node5': 1.0, 'doc1_node12': 0.8},
            category='medical'
        ))
        
        # ì‹¤í–‰
        results = framework.run_benchmark(tree_rag_system, flat_rag_system)
        
        # ë¦¬í¬íŠ¸ ìƒì„±
        report = framework.generate_report(results)
    """
    
    def __init__(self):
        self.test_cases: List[QueryTestCase] = []
        self.results: List[BenchmarkResult] = []
    
    def add_test_case(self, test_case: QueryTestCase):
        """í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì¶”ê°€"""
        self.test_cases.append(test_case)
    
    def add_test_cases_from_json(self, json_path: str):
        """
        JSON íŒŒì¼ì—ì„œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì¼ê´„ ë¡œë“œ
        
        Format:
        {
            "test_cases": [
                {
                    "query": "...",
                    "relevant_docs": ["doc1", "doc2"],
                    "relevant_scores": {"doc1": 1.0, "doc2": 0.8},
                    "expected_citations": ["doc1#p10"],
                    "category": "medical",
                    "domain": "medical"
                }
            ]
        }
        """
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        for tc in data.get('test_cases', []):
            self.add_test_case(QueryTestCase(**tc))
        
        print(f"âœ… Loaded {len(data.get('test_cases', []))} test cases from {json_path}")
    
    def run_single_query(
        self,
        system: Any,
        test_case: QueryTestCase,
        system_name: str
    ) -> BenchmarkResult:
        """
        ë‹¨ì¼ ì¿¼ë¦¬ ì‹¤í–‰ ë° í‰ê°€
        
        Args:
            system: TreeRAGReasoner ë˜ëŠ” FlatRAGBaseline ì¸ìŠ¤í„´ìŠ¤
            test_case: í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
            system_name: 'TreeRAG' ë˜ëŠ” 'FlatRAG'
            
        Returns:
            BenchmarkResult
        """
        print(f"  ðŸ” Query: {test_case.query[:60]}...")
        
        start_time = time.time()
        
        try:
            answer, metadata = system.query(
                test_case.query,
                max_depth=5,
                max_branches=3
            )
            
            latency_ms = (time.time() - start_time) * 1000
            
            retrieved_docs = self._extract_retrieved_docs(metadata)
            context_size = metadata.get('context_size', 0)
            
        except Exception as e:
            print(f"    âš ï¸ Error: {e}")
            return BenchmarkResult(
                test_case=test_case,
                system_name=system_name,
                retrieved_docs=[],
                generated_answer="",
                latency_ms=0.0,
                context_size=0,
                metrics={
                    'precision@3': 0.0,
                    'recall@3': 0.0,
                    'f1@3': 0.0,
                    'error': str(e)
                }
            )
        
        metrics = self._calculate_metrics(
            test_case=test_case,
            retrieved_docs=retrieved_docs,
            generated_answer=answer,
            context_size=context_size
        )
        
        print(f"    âœ… P@3={metrics['precision@3']:.3f}, R@3={metrics['recall@3']:.3f}, "
              f"F1@3={metrics['f1@3']:.3f}, {latency_ms:.0f}ms")
        
        return BenchmarkResult(
            test_case=test_case,
            system_name=system_name,
            retrieved_docs=retrieved_docs,
            generated_answer=answer,
            latency_ms=latency_ms,
            context_size=context_size,
            metrics=metrics
        )
    
    def run_benchmark(
        self,
        tree_rag_system: Any,
        flat_rag_system: Optional[Any] = None,
        save_results: bool = True,
        output_dir: str = "benchmark_results"
    ) -> Dict[str, List[BenchmarkResult]]:
        """
        ì „ì²´ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
        
        Args:
            tree_rag_system: TreeRAGReasoner ì¸ìŠ¤í„´ìŠ¤
            flat_rag_system: FlatRAGBaseline ì¸ìŠ¤í„´ìŠ¤ (Noneì´ë©´ TreeRAGë§Œ í…ŒìŠ¤íŠ¸)
            save_results: ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ì €ìž¥í• ì§€ ì—¬ë¶€
            output_dir: ê²°ê³¼ ì €ìž¥ ë””ë ‰í† ë¦¬
            
        Returns:
            {
                'TreeRAG': [BenchmarkResult, ...],
                'FlatRAG': [BenchmarkResult, ...]  # flat_rag_systemì´ ìžˆëŠ” ê²½ìš°ë§Œ
            }
        """
        if not self.test_cases:
            raise ValueError("No test cases added. Use add_test_case() first.")
        
        print(f"\n{'='*60}")
        print(f"ðŸš€ Starting Benchmark with {len(self.test_cases)} test cases")
        print(f"{'='*60}\n")
        
        results = {'TreeRAG': []}
        
        print("ðŸ“Š Evaluating TreeRAG...")
        for i, test_case in enumerate(self.test_cases, 1):
            print(f"[{i}/{len(self.test_cases)}]", end=" ")
            result = self.run_single_query(tree_rag_system, test_case, 'TreeRAG')
            results['TreeRAG'].append(result)
        
        if flat_rag_system:
            results['FlatRAG'] = []
            print(f"\nðŸ“Š Evaluating FlatRAG...")
            for i, test_case in enumerate(self.test_cases, 1):
                print(f"[{i}/{len(self.test_cases)}]", end=" ")
                result = self.run_single_query(flat_rag_system, test_case, 'FlatRAG')
                results['FlatRAG'].append(result)
        
        print(f"\n{'='*60}")
        print("âœ… Benchmark completed!")
        print(f"{'='*60}\n")
        
        if save_results:
            self._save_results(results, output_dir)
        
        self.results = results
        return results
    
    def _extract_retrieved_docs(self, metadata: Dict[str, Any]) -> List[str]:
        """ë©”íƒ€ë°ì´í„°ì—ì„œ ê²€ìƒ‰ëœ ë¬¸ì„œ ID ì¶”ì¶œ"""
        if 'traversal_info' in metadata:
            nodes = metadata['traversal_info'].get('nodes_selected', [])
            return [node.get('node', {}).get('id', '') for node in nodes]
        
        if 'retrieved_docs' in metadata:
            return metadata['retrieved_docs']
        
        return []
    
    def _calculate_metrics(
        self,
        test_case: QueryTestCase,
        retrieved_docs: List[str],
        generated_answer: str,
        context_size: int
    ) -> Dict[str, float]:
        """ê° ê²°ê³¼ì— ëŒ€í•œ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        
        relevant_set = set(test_case.relevant_docs)
        metrics = {}
        
        for k in [1, 3, 5]:
            metrics[f'precision@{k}'] = EvaluationMetrics.precision_at_k(
                retrieved_docs, relevant_set, k
            )
            metrics[f'recall@{k}'] = EvaluationMetrics.recall_at_k(
                retrieved_docs, relevant_set, k
            )
            metrics[f'f1@{k}'] = EvaluationMetrics.f1_at_k(
                retrieved_docs, relevant_set, k
            )
        
        if test_case.relevant_scores:
            for k in [3, 5]:
                metrics[f'ndcg@{k}'] = EvaluationMetrics.ndcg_at_k(
                    retrieved_docs, test_case.relevant_scores, k
                )
        
        if test_case.expected_citations:
            citation_acc, citation_details = EvaluationMetrics.citation_accuracy(
                generated_answer,
                set(test_case.expected_citations)
            )
            metrics['citation_accuracy'] = citation_acc
            metrics['citations_found'] = citation_details['correct']
            metrics['citations_missing'] = citation_details['missing']
        
        if retrieved_docs and generated_answer:
            metrics['answer_length'] = len(generated_answer)
        
        metrics['context_size'] = context_size
        
        return metrics
    
    def _save_results(self, results: Dict[str, List[BenchmarkResult]], output_dir: str):
        """ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ìž¥"""
        os.makedirs(output_dir, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"benchmark_results_{timestamp}.json"
        filepath = os.path.join(output_dir, filename)
        
        serializable_results = {}
        for system_name, result_list in results.items():
            serializable_results[system_name] = [r.to_dict() for r in result_list]
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(serializable_results, f, indent=2, ensure_ascii=False)
        
        print(f"ðŸ’¾ Results saved to: {filepath}")
    
    def generate_report(
        self,
        results: Optional[Dict[str, List[BenchmarkResult]]] = None
    ) -> str:
        """
        ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±
        
        Args:
            results: run_benchmark()ì˜ ê²°ê³¼ (Noneì´ë©´ self.results ì‚¬ìš©)
            
        Returns:
            ì‚¬ëžŒì´ ì½ê¸° ì‰¬ìš´ í…ìŠ¤íŠ¸ ë¦¬í¬íŠ¸
        """
        if results is None:
            results = self.results
        
        if not results:
            return "No results to report. Run benchmark first."
        
        report = []
        report.append("=" * 80)
        report.append("TreeRAG COMPREHENSIVE BENCHMARK REPORT")
        report.append("=" * 80)
        report.append(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append(f"Test Cases: {len(self.test_cases)}")
        report.append("")
        
        for system_name, result_list in results.items():
            report.append("-" * 80)
            report.append(f"ðŸ“Š {system_name} Results")
            report.append("-" * 80)
            
            all_metrics = [r.metrics for r in result_list]
            aggregated = EvaluationMetrics.aggregate_metrics(all_metrics)
            
            # Retrieval Quality
            report.append("\nðŸŽ¯ Retrieval Quality:")
            for metric_name in ['precision@1', 'precision@3', 'precision@5',
                                'recall@1', 'recall@3', 'recall@5',
                                'f1@1', 'f1@3', 'f1@5']:
                if metric_name in aggregated:
                    stats = aggregated[metric_name]
                    report.append(
                        f"  {metric_name:20s}: {stats['mean']:.4f} "
                        f"(Â±{stats['std']:.4f}) "
                        f"[{stats['min']:.4f} - {stats['max']:.4f}]"
                    )
            
            # NDCG
            if 'ndcg@3' in aggregated or 'ndcg@5' in aggregated:
                report.append("\nðŸ“ˆ Ranking Quality (NDCG):")
                for metric_name in ['ndcg@3', 'ndcg@5']:
                    if metric_name in aggregated:
                        stats = aggregated[metric_name]
                        report.append(
                            f"  {metric_name:20s}: {stats['mean']:.4f} "
                            f"(Â±{stats['std']:.4f})"
                        )
            
            # Citation Accuracy
            if 'citation_accuracy' in aggregated:
                report.append("\nðŸ“ Citation Quality:")
                stats = aggregated['citation_accuracy']
                report.append(
                    f"  Citation Accuracy    : {stats['mean']:.4f} "
                    f"(Â±{stats['std']:.4f})"
                )
            
            # Efficiency
            report.append("\nâš¡ Efficiency:")
            total_latency = sum(r.latency_ms for r in result_list)
            avg_latency = total_latency / len(result_list) if result_list else 0
            report.append(f"  Avg Latency          : {avg_latency:.2f} ms")
            report.append(f"  Total Time           : {total_latency/1000:.2f} s")
            
            if 'context_size' in aggregated:
                stats = aggregated['context_size']
                report.append(
                    f"  Avg Context Size     : {stats['mean']:.0f} tokens "
                    f"(Â±{stats['std']:.0f})"
                )
            
            report.append("")
        
        if 'TreeRAG' in results and 'FlatRAG' in results:
            report.append("-" * 80)
            report.append("âš”ï¸  TreeRAG vs FlatRAG Comparison")
            report.append("-" * 80)
            
            tree_metrics = EvaluationMetrics.aggregate_metrics(
                [r.metrics for r in results['TreeRAG']]
            )
            flat_metrics = EvaluationMetrics.aggregate_metrics(
                [r.metrics for r in results['FlatRAG']]
            )
            
            for metric_name in ['precision@3', 'recall@3', 'f1@3']:
                if metric_name in tree_metrics and metric_name in flat_metrics:
                    tree_val = tree_metrics[metric_name]['mean']
                    flat_val = flat_metrics[metric_name]['mean']
                    diff = tree_val - flat_val
                    symbol = "ðŸŸ¢" if diff > 0 else "ðŸ”´" if diff < 0 else "ðŸŸ¡"
                    report.append(
                        f"{symbol} {metric_name:20s}: TreeRAG={tree_val:.4f} vs "
                        f"FlatRAG={flat_val:.4f} (Î”={diff:+.4f})"
                    )
            
            if 'context_size' in tree_metrics and 'context_size' in flat_metrics:
                tree_ctx = tree_metrics['context_size']['mean']
                flat_ctx = flat_metrics['context_size']['mean']
                reduction = EvaluationMetrics.context_reduction_rate(flat_ctx, tree_ctx)
                report.append(
                    f"\nðŸ’¡ Context Reduction  : {reduction*100:.1f}% "
                    f"({flat_ctx:.0f} â†’ {tree_ctx:.0f} tokens)"
                )
            
            tree_latency = sum(r.latency_ms for r in results['TreeRAG']) / len(results['TreeRAG'])
            flat_latency = sum(r.latency_ms for r in results['FlatRAG']) / len(results['FlatRAG'])
            latency_comp = EvaluationMetrics.latency_comparison(tree_latency, flat_latency)
            
            report.append(
                f"âš¡ Latency            : TreeRAG={tree_latency:.2f}ms vs "
                f"FlatRAG={flat_latency:.2f}ms "
                f"({latency_comp['speedup']:.2f}x {latency_comp['faster_system']})"
            )
        
        report.append("")
        report.append("=" * 80)
        
        return "\n".join(report)
    
    def generate_comparison_table(
        self,
        results: Optional[Dict[str, List[BenchmarkResult]]] = None,
        output_format: str = 'markdown'
    ) -> str:
        """
        TreeRAG vs FlatRAG ë¹„êµ í…Œì´ë¸” ìƒì„±
        
        Args:
            results: ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼
            output_format: 'markdown' ë˜ëŠ” 'latex'
            
        Returns:
            ë¹„êµ í…Œì´ë¸” (ë§ˆí¬ë‹¤ìš´ ë˜ëŠ” LaTeX í˜•ì‹)
        """
        if results is None:
            results = self.results
        
        if 'TreeRAG' not in results or 'FlatRAG' not in results:
            return "Cannot generate comparison table without both TreeRAG and FlatRAG results."
        
        tree_metrics = EvaluationMetrics.aggregate_metrics(
            [r.metrics for r in results['TreeRAG']]
        )
        flat_metrics = EvaluationMetrics.aggregate_metrics(
            [r.metrics for r in results['FlatRAG']]
        )
        
        if output_format == 'markdown':
            return self._comparison_table_markdown(tree_metrics, flat_metrics, results)
        elif output_format == 'latex':
            return self._comparison_table_latex(tree_metrics, flat_metrics, results)
        else:
            raise ValueError(f"Unknown format: {output_format}")
    
    def _comparison_table_markdown(
        self,
        tree_metrics: Dict,
        flat_metrics: Dict,
        results: Dict
    ) -> str:
        """ë§ˆí¬ë‹¤ìš´ ë¹„êµ í…Œì´ë¸” ìƒì„±"""
        table = []
        table.append("| Metric | TreeRAG | FlatRAG | Improvement |")
        table.append("|--------|---------|---------|-------------|")
        
        for metric_name in ['precision@3', 'recall@3', 'f1@3', 'ndcg@3']:
            if metric_name in tree_metrics and metric_name in flat_metrics:
                tree_val = tree_metrics[metric_name]['mean']
                flat_val = flat_metrics[metric_name]['mean']
                improvement = ((tree_val - flat_val) / flat_val * 100) if flat_val > 0 else 0
                
                table.append(
                    f"| {metric_name} | {tree_val:.4f} | {flat_val:.4f} | "
                    f"{improvement:+.1f}% |"
                )
        
        if 'context_size' in tree_metrics and 'context_size' in flat_metrics:
            tree_ctx = tree_metrics['context_size']['mean']
            flat_ctx = flat_metrics['context_size']['mean']
            reduction = (1 - tree_ctx/flat_ctx) * 100 if flat_ctx > 0 else 0
            
            table.append(
                f"| Context Size (tokens) | {tree_ctx:.0f} | {flat_ctx:.0f} | "
                f"{reduction:.1f}% reduction |"
            )
        
        tree_latency = sum(r.latency_ms for r in results['TreeRAG']) / len(results['TreeRAG'])
        flat_latency = sum(r.latency_ms for r in results['FlatRAG']) / len(results['FlatRAG'])
        latency_improvement = ((flat_latency - tree_latency) / flat_latency * 100) if flat_latency > 0 else 0
        
        table.append(
            f"| Latency (ms) | {tree_latency:.2f} | {flat_latency:.2f} | "
            f"{latency_improvement:+.1f}% |"
        )
        
        return "\n".join(table)
    
    def _comparison_table_latex(
        self,
        tree_metrics: Dict,
        flat_metrics: Dict,
        results: Dict
    ) -> str:
        """LaTeX ë¹„êµ í…Œì´ë¸” ìƒì„± (ë…¼ë¬¸ìš©)"""
        table = []
        table.append("\\begin{table}[h]")
        table.append("\\centering")
        table.append("\\caption{TreeRAG vs Flat RAG Performance Comparison}")
        table.append("\\begin{tabular}{lccc}")
        table.append("\\hline")
        table.append("Metric & TreeRAG & Flat RAG & Improvement \\\\")
        table.append("\\hline")
        
        for metric_name in ['precision@3', 'recall@3', 'f1@3']:
            if metric_name in tree_metrics and metric_name in flat_metrics:
                tree_val = tree_metrics[metric_name]['mean']
                flat_val = flat_metrics[metric_name]['mean']
                improvement = ((tree_val - flat_val) / flat_val * 100) if flat_val > 0 else 0
                
                name_display = metric_name.replace('@', '@')
                table.append(
                    f"{name_display} & {tree_val:.4f} & {flat_val:.4f} & "
                    f"{improvement:+.1f}\\% \\\\"
                )
        
        table.append("\\hline")
        table.append("\\end{tabular}")
        table.append("\\end{table}")
        
        return "\n".join(table)
