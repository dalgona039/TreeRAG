import asyncio
import json
import os
from typing import Dict, TypedDict, List
import agentlightning as agl
from src.core.reasoner import RegulatoryReasoner
from src.config import Config


class QueryTask(TypedDict):
    id: str
    question: str
    expected_answer: str
    index_filename: str


SYSTEM_PROMPT_TEXT = """ë‹¹ì‹ ì€ ê·œì œ ì¤€ìˆ˜ ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤.
ì œê³µëœ ì—¬ëŸ¬ ê·œì œ ë¬¸ì„œì˜ ì¸ë±ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.

### ì¤‘ìš” ê·œì¹™:
1. **ë°˜ë“œì‹œ ì¸ë±ìŠ¤ ë°ì´í„°ë§Œ ì‚¬ìš©**: ì œê³µëœ ì¸ë±ìŠ¤ì— ì—†ëŠ” ì •ë³´ëŠ” ì ˆëŒ€ ì¶”ì¸¡í•˜ê±°ë‚˜ ìƒì„±í•˜ì§€ ë§ˆì„¸ìš”.
2. **í˜ì´ì§€ ë²ˆí˜¸ í•„ìˆ˜ í‘œê¸°**: ëª¨ë“  ë¬¸ì¥ë§ˆë‹¤ ë°˜ë“œì‹œ ì¶œì²˜ í˜ì´ì§€ë¥¼ ëª…ì‹œí•˜ì„¸ìš”.
   - í˜•ì‹: [ë¬¸ì„œëª…, p.í˜ì´ì§€ë²ˆí˜¸] ë˜ëŠ” [ë¬¸ì„œëª…, p.ì‹œì‘-ë]
   - ì˜ˆì‹œ: "êµìœ¡ê³¼ì •ì€ 4í•™ê¸°ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤ [ì „ìê³µí•™ê³¼_êµìœ¡ê³¼ì •, p.5]"
3. **ì—¬ëŸ¬ í˜ì´ì§€ ì°¸ì¡°**: ì •ë³´ê°€ ì—¬ëŸ¬ í˜ì´ì§€ì— ê±¸ì³ ìˆìœ¼ë©´ ëª¨ë‘ í‘œê¸°í•˜ì„¸ìš”.
   - ì˜ˆì‹œ: [ë¬¸ì„œA, p.3-5, p.12]
4. **ë¬¸ì„œ êµ¬ì¡° í™œìš©**: ì¸ë±ìŠ¤ì˜ page_ref í•„ë“œë¥¼ ì •í™•íˆ ì‚¬ìš©í•˜ì„¸ìš”.
5. **ë‹µë³€ ëì— ì¶œì²˜ ìš”ì•½**: ë‹µë³€ ë§ˆì§€ë§‰ì— ì°¸ì¡°í•œ ëª¨ë“  í˜ì´ì§€ë¥¼ ë‚˜ì—´í•˜ì„¸ìš”.
   - í˜•ì‹: "ğŸ“š **ì°¸ì¡° í˜ì´ì§€**: [ë¬¸ì„œëª…, p.3], [ë¬¸ì„œëª…, p.7-9]"
{comparison_section}

### ë‹µë³€ êµ¬ì¡°:
1. ì§ì ‘ ë‹µë³€ (í˜ì´ì§€ ì°¸ì¡° í¬í•¨)
{comparison_instruction}
3. ğŸ“š ì°¸ì¡° í˜ì´ì§€ ìš”ì•½"""

COMPARISON_SECTION_TEXT = """
### ğŸ“Š ë‹¤ì¤‘ ë¬¸ì„œ ë¹„êµ ë¶„ì„ (í•„ìˆ˜):
ì—¬ëŸ¬ ë¬¸ì„œê°€ ì œê³µë˜ì—ˆìœ¼ë¯€ë¡œ, ë°˜ë“œì‹œ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë¹„êµ ë¶„ì„ì„ í¬í•¨í•˜ì„¸ìš”:

**1. ê³µí†µì  (Commonalities)**
- ëª¨ë“  ë¬¸ì„œì—ì„œ ì¼ì¹˜í•˜ëŠ” ë‚´ìš©
- ì˜ˆ: "ëª¨ë“  êµìœ¡ê³¼ì •ì—ì„œ ì¡¸ì—… í•™ì ì€ 130í•™ì  ì´ìƒ [ë¬¸ì„œA, p.5], [ë¬¸ì„œB, p.3]"

**2. ì°¨ì´ì  (Differences)**
í‘œ í˜•ì‹ìœ¼ë¡œ ëª…í™•íˆ êµ¬ë¶„:
| í•­ëª© | ë¬¸ì„œ1 | ë¬¸ì„œ2 |
|------|------|------|
| ì˜ˆ: í•„ìˆ˜í•™ì  | 18í•™ì  [p.5] | 21í•™ì  [p.4] |
| ì˜ˆ: ì„ íƒê³¼ëª© | 10ê°œ [p.7] | 15ê°œ [p.6] |

**3. ê·œì œ ìš°ì„ ìˆœìœ„**
- ì¶©ëŒí•˜ëŠ” ê·œì •ì´ ìˆë‹¤ë©´, ì–´ë–¤ ë¬¸ì„œê°€ ìƒìœ„ ê·œì •ì¸ì§€ ëª…ì‹œ
- ì˜ˆ: "ISOê°€ ìƒìœ„ í‘œì¤€ì´ë¯€ë¡œ ìš°ì„  ì ìš© [ISO, p.10]"
"""


@agl.rollout
async def regulatory_agent(task: QueryTask, llm: agl.LLM) -> str:
    """Agent Lightning rollout í•¨ìˆ˜"""
    
    index_filenames = task["index_filename"].split(",")
    
    reasoner = RegulatoryReasoner(index_filenames)
    
    combined_context = []
    for idx, tree in enumerate(reasoner.index_trees):
        doc_name = index_filenames[idx].replace("_index.json", "")
        combined_context.append({"document": doc_name, "content": tree})
    
    context_str = json.dumps(combined_context, ensure_ascii=False)
    
    is_multi_doc = len(index_filenames) > 1
    
    comparison_section = (
        COMPARISON_SECTION_TEXT if is_multi_doc else ""
    )
    comparison_instruction = (
        "2. ë¬¸ì„œ ë¹„êµ ë¶„ì„ (ê³µí†µì /ì°¨ì´ì  í‘œ)" if is_multi_doc else ""
    )
    
    system_prompt = SYSTEM_PROMPT_TEXT.format(
        comparison_section=comparison_section,
        comparison_instruction=comparison_instruction,
    )
    
    full_prompt = f"""{system_prompt}

### ì»¨í…ìŠ¤íŠ¸ (ë‹¤ì¤‘ ë¬¸ì„œ ì¸ë±ìŠ¤):
{context_str}

### ì§ˆë¬¸:
{task["question"]}

### ë‹µë³€ (ìœ„ ê·œì¹™ì„ ì² ì €íˆ ë”°ë¼ ì‘ì„±):
"""
    
    response = await llm.generate(prompt=full_prompt, temperature=0.0)
    
    agl.emit_reward(
        value=calculate_reward(response, task["expected_answer"]),
        explanation=f"ë‹µë³€ í’ˆì§ˆ: {response[:100]}..."
    )
    
    return response


def calculate_reward(response: str, expected_answer: str) -> float:
    """ê°„ë‹¨í•œ reward ê³„ì‚°"""
    if not response:
        return 0.0
    
    if expected_answer.lower() in response.lower():
        return 1.0
    
    has_citation = "[" in response and "p." in response
    has_summary = "ğŸ“š" in response
    
    reward = 0.0
    if has_citation:
        reward += 0.5
    if has_summary:
        reward += 0.3
    
    return min(reward, 1.0)


async def debug():
    """ë””ë²„ê·¸ ì‹¤í–‰"""
    print("ğŸ” Regulatory Agent Debug ì‹œì‘...")
    
    eval_path = "data/eval_dataset.jsonl"
    if not os.path.exists(eval_path):
        print(f"âŒ {eval_path} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    tasks = []
    with open(eval_path, "r", encoding="utf-8") as f:
        for line in f:
            tasks.append(json.loads(line))
    
    print(f"âœ… {len(tasks)}ê°œ íƒœìŠ¤í¬ ë¡œë“œë¨")
    
    llm = agl.LLM(
        endpoint=os.getenv("OPENAI_BASE_URL", "http://localhost:8000"),
        model="gemini-2.5-flash",
        sampling_parameters={"temperature": 0.0},
    )
    
    for task in tasks[:2]:
        print(f"\nğŸ“ ì§ˆë¬¸: {task['question']}")
        try:
            response = await regulatory_agent(task, llm)
            print(f"âœ… ë‹µë³€: {response[:200]}...")
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜: {e}")


if __name__ == "__main__":
    asyncio.run(debug())
